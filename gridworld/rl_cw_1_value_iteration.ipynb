{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fbbb4c579309cc8ef75c0e632529bc7",
     "grade": false,
     "grade_id": "cell-33b0e4dce2016840",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 1: Value Iteration\n",
    "\n",
    "In this exercise, you will implement the value iteration algorithm for three closely related, but different, gridworld environments.\n",
    "\n",
    "**Total number of marks:** 20 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include **all** of your source code. Please **do not change the file name or compress/zip your submission**. Please do not include any identifying information on the files you submit. This coursework will be marked **anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and **write your own code**.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please **do not use any non-standard, third-party libraries** apart from numpy and matplotlib. **If we are unable to run your code because you have used unsupported external libraries, you may not get any credit for your work.**\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **1 minute** on the University's computers. Otherwise, you may not get credit for your work. You can run your code on the university's computers remotely using [UniDesk](https://bath.topdesk.net/tas/public/ssp/content/detail/knowledgeitem?unid=ff3266344c1d4eb2acb227cc9e3e1eee)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1f63de0fdcf845091a97902c847d2e",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "In this coursework, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the lectures. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to $1 \\times 10 ^{-10}$. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42c1c34d1fb04bef22da8276ff9780c",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (0 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations `\"n\"`, `\"e\"`, `\"s\"`, and `\"w\"` for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea5b17f9f7aea9b1f962fc29862764bc",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0 8.0 9.0 0.0 9.0\n",
      "6.0 7.0 8.0 0.0 8.0\n",
      "5.0 6.0 7.0 6.0 7.0\n",
      "4.0 5.0 6.0 5.0 6.0\n",
      "3.0 4.0 5.0 4.0 5.0\n",
      "e e e n w\n",
      "n n n n n\n",
      "n n n e n\n",
      "n n n n n\n",
      "n n n n n\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "import numpy as np\n",
    "\n",
    "class Deterministic:\n",
    "    def __init__(self):\n",
    "        # gridworld dimensions\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "\n",
    "        # item positions\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.terminal_states = np.array([self.bomb_positions, self.gold_positions])\n",
    "        \n",
    "        # reward is -1 for moving into grid square with an additional reward of +/- 10 for the terminal states\n",
    "        self.rewards = np.array([-1]*self.num_cells)\n",
    "        self.rewards[self.bomb_positions] += -10\n",
    "        self.rewards[self.gold_positions] += 10\n",
    "\n",
    "        self.available_actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "\n",
    "        # state values are initially zero and policy has been initialised with optimal policy \"n\"\n",
    "        self.v = np.zeros(self.num_cells)\n",
    "        self.policy = np.array([\"n\"]*self.num_cells)\n",
    "        self.theta = 1e-10\n",
    "        self.gamma = 1\n",
    "\n",
    "    def find_max_v_action(self, state_v):\n",
    "        \"\"\"\n",
    "        uses the values and rewards for neighbouring squares, and\n",
    "        returns action corresponding to the highest value.\n",
    "        \"\"\"\n",
    "        neighbour_v, neighbour_r = self.neighbour_v_r_values(state_v)\n",
    "        north = neighbour_r[0] + self.gamma * neighbour_v[0]\n",
    "        east = neighbour_r[1] + self.gamma * neighbour_v[1]\n",
    "        south = neighbour_r[2] + self.gamma * neighbour_v[2]\n",
    "        west = neighbour_r[3] + self.gamma * neighbour_v[3]\n",
    "        max_v = max([north, east, south, west])\n",
    "        action_index = [north, east, south, west].index(max_v)\n",
    "        return max_v, action_index\n",
    "\n",
    "    def neighbour_v_r_values(self, state):\n",
    "        \"\"\"\n",
    "        returns 2 lists for value and rewards for neighbouring squares\n",
    "        \"\"\"\n",
    "        if (state + self.num_cols) < self.num_cells:\n",
    "            v_n = self.v[state + self.num_cols]\n",
    "            r_n = self.rewards[state + self.num_cols]\n",
    "        else:\n",
    "            v_n = self.v[state]\n",
    "            r_n = self.rewards[state]\n",
    "        if (state + 1) % self.num_cols > 0:\n",
    "            v_e = self.v[state + 1]\n",
    "            r_e = self.rewards[state + 1]\n",
    "        else:\n",
    "            v_e = self.v[state]\n",
    "            r_e = self.rewards[state]\n",
    "        if (state - self.num_cols) >= 0:\n",
    "            v_s = self.v[state - self.num_cols]\n",
    "            r_s = self.rewards[state - self.num_cols]\n",
    "        else:\n",
    "            v_s = self.v[state]\n",
    "            r_s = self.rewards[state]\n",
    "        if (state - 1) % self.num_cols < self.num_cols - 1:\n",
    "            v_w = self.v[state - 1]\n",
    "            r_w = self.rewards[state - 1]\n",
    "        else:\n",
    "            v_w = self.v[state]\n",
    "            r_w = self.rewards[state]\n",
    "        return [v_n, v_e, v_s, v_w], [r_n, r_e, r_s, r_w]\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        applies value iteration algorithm to iterate over all non-terminal states until delta < theta\n",
    "        \"\"\"\n",
    "        delta = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(0, len(self.v)):\n",
    "                if s not in self.terminal_states:\n",
    "                    state_v = self.v[s]\n",
    "                    max_v, action = self.find_max_v_action(s)\n",
    "                    self.v[s] = max_v\n",
    "                    self.policy[s] = self.available_actions[action]\n",
    "                    delta = max([delta, abs(self.v[s] - state_v)])        \n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "        return self.v, self.policy\n",
    "\n",
    "env1 = Deterministic()\n",
    "v, policy = env1.value_iteration()\n",
    "\n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(v[i], v[i+1], v[i+2], v[i+3], v[i+4]))\n",
    "    \n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(policy[i], policy[i+1], policy[i+2], policy[i+3], policy[i+4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f6a967ae761f92bf2a79af2be971939",
     "grade": false,
     "grade_id": "cell-02a5c34a5b828d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Example Test Cell**\n",
    "\n",
    "In the code cell below, we have provided an example of the type of test code that we will use to mark your work. In these tests, we first check that your `policy` and `v` variables are of the correct type, and then check that their values match the solution. In the future, the test code will be hidden from you.\n",
    "\n",
    "You must not delete or modify test cells in any way - any modifications you do make will be overwritten at run-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e11d89ae67ea8af3e01f18e48bd35f6",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Solution policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Student's v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "Solution v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We're giving you the solution values for Exercise 1, but not telling you how to compute them!\n",
    "solution_values = [3.0, 4.0, 5.0, 4.0, 5.0,\n",
    "                   4.0, 5.0, 6.0, 5.0, 6.0,\n",
    "                   5.0, 6.0, 7.0, 6.0, 7.0,\n",
    "                   6.0, 7.0, 8.0, 0.0, 8.0,\n",
    "                   7.0, 8.0, 9.0, 0.0, 9.0]\n",
    "solution_values = np.array(solution_values)\n",
    "\n",
    "# We're giving you the solution policy for Exercise 1, but not telling you how to compute it!\n",
    "solution_policy = [\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'e', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'e', 'e', 'e', 'n', 'w',]\n",
    "solution_policy = np.array(solution_policy)\n",
    "\n",
    "# Check that policy and v are numpy arrays.\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether policy contains only \"n\", \"w\", \"s\", or \"e\" values.\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))\n",
    "\n",
    "# Print student's solution and true solution for easier comparison / spotting of errors.\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))\n",
    "print(\"Solution policy:\")\n",
    "print(np.flip(solution_policy.reshape((5, 5)), 0))\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"Solution v:\")\n",
    "print(np.flip(solution_values.reshape((5, 5)), 0))\n",
    "\n",
    "# Compare policy (only on states that have a single optimal direction).\n",
    "states_to_check =  np.array([4, 9, 14, 17, 19, 20, 22, 24])\n",
    "np.testing.assert_array_equal(policy[states_to_check], solution_policy[states_to_check])\n",
    "\n",
    "# Compare state_values (also for terminal states --- they have to be zero!).\n",
    "states_to_check = np.delete(np.arange(25), np.array([18, 23]))\n",
    "np.testing.assert_array_almost_equal(v[states_to_check], solution_values[states_to_check], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db00038a44803605ff0fe1f8e0971696",
     "grade": false,
     "grade_id": "cell-05eb78b7446cb694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment (12 Marks)\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596c020f7ef0ae864054c8dbc3496cc2",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0416932891286494 7.2875663582680215 8.613599508716593 0.0 8.692623107335235\n",
      "4.861851113759854 5.990875869781758 6.370824307347232 0.0 6.467215932034224\n",
      "3.6755093764709437 4.696213883972035 4.994418628980848 3.2189157994407624 5.102509883951255\n",
      "2.486995335069779 3.409459887700942 3.669229671301549 2.6412293326632748 3.7861011510512865\n",
      "1.35979207866988 2.197336720132774 2.4287875129945746 1.5727216146527712 2.552024510140129\n",
      "e e e n w\n",
      "n n n n n\n",
      "n n n e n\n",
      "n n n e n\n",
      "n n n n n\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "import numpy as np\n",
    "\n",
    "class Stochastic:\n",
    "    def __init__(self):\n",
    "        # gridworld dimensions\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "\n",
    "        # item positions\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.terminal_states = np.array([self.bomb_positions, self.gold_positions])\n",
    "        \n",
    "        # reward is -1 for moving into grid square with an additional reward of +/- 10 for the terminal states\n",
    "        self.rewards = np.array([-1]*self.num_cells)\n",
    "        self.rewards[self.bomb_positions] += -10\n",
    "        self.rewards[self.gold_positions] += 10\n",
    "\n",
    "        self.available_actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "\n",
    "        # state values are initially zero and policy has been initialised with optimal policy \"n\"\n",
    "        self.v = np.zeros(self.num_cells)\n",
    "        self.policy = np.array([\"n\"]*self.num_cells)\n",
    "        self.theta = 1e-10\n",
    "        self.gamma = 1\n",
    "        # probability of agent not moving as intended is 0.2\n",
    "        self.unintended = 0.2\n",
    "        self.intended = 1 - 0.2\n",
    "\n",
    "    def find_max_v_action(self, state_v):\n",
    "        \"\"\"\n",
    "        uses the values and rewards for neighbouring squares, and returns action corresponding to \n",
    "        the highest value. In stochastic environment, value of each direction is 0.8 probability \n",
    "        intended action and 0.2 random action (with 0.25 chance of each of the 4 actions\n",
    "        \"\"\"\n",
    "        neighbour_v, neighbour_r = self.neighbour_v_r_values(state_v)\n",
    "        north = neighbour_r[0] + self.gamma * neighbour_v[0]\n",
    "        east = neighbour_r[1] + self.gamma * neighbour_v[1]\n",
    "        south = neighbour_r[2] + self.gamma * neighbour_v[2]\n",
    "        west = neighbour_r[3] + self.gamma * neighbour_v[3]\n",
    "        # for stochastic environment, 0.8 prob of intended action and 0.2 prob of random action\n",
    "        st_north = self.intended*north+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_east = self.intended*east+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_south = self.intended*south+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_west = self.intended*west+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        max_v = max([st_north, st_east, st_south, st_west])\n",
    "        action_index = [st_north, st_east, st_south, st_west].index(max_v)\n",
    "        return max_v, action_index\n",
    "\n",
    "    def neighbour_v_r_values(self, state):\n",
    "        \"\"\"\n",
    "        returns 2 lists for value and rewards for neighbouring squares\n",
    "        \"\"\"\n",
    "        if (state + self.num_cols) < self.num_cells:\n",
    "            v_n = self.v[state + self.num_cols]\n",
    "            r_n = self.rewards[state + self.num_cols]\n",
    "        else:\n",
    "            v_n = self.v[state]\n",
    "            r_n = self.rewards[state]\n",
    "        if (state + 1) % self.num_cols > 0:\n",
    "            v_e = self.v[state + 1]\n",
    "            r_e = self.rewards[state + 1]\n",
    "        else:\n",
    "            v_e = self.v[state]\n",
    "            r_e = self.rewards[state]\n",
    "        if (state - self.num_cols) >= 0:\n",
    "            v_s = self.v[state - self.num_cols]\n",
    "            r_s = self.rewards[state - self.num_cols]\n",
    "        else:\n",
    "            v_s = self.v[state]\n",
    "            r_s = self.rewards[state]\n",
    "        if (state - 1) % self.num_cols < self.num_cols - 1:\n",
    "            v_w = self.v[state - 1]\n",
    "            r_w = self.rewards[state - 1]\n",
    "        else:\n",
    "            v_w = self.v[state]\n",
    "            r_w = self.rewards[state]\n",
    "        return [v_n, v_e, v_s, v_w], [r_n, r_e, r_s, r_w]\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        applies value iteration algorithm to iterate over all non-terminal states until delta < theta\n",
    "        \"\"\"\n",
    "        delta = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(0, len(self.v)):\n",
    "                if s not in self.terminal_states:\n",
    "                    state_v = self.v[s]\n",
    "                    max_v, action = self.find_max_v_action(s)\n",
    "                    self.v[s] = max_v\n",
    "                    self.policy[s] = self.available_actions[action]\n",
    "                    delta = max([delta, abs(self.v[s] - state_v)])     \n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "        return self.v, self.policy\n",
    "\n",
    "env2 = Stochastic()\n",
    "v, policy = env2.value_iteration()\n",
    "\n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(v[i], v[i+1], v[i+2], v[i+3], v[i+4]))\n",
    "    \n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(policy[i], policy[i+1], policy[i+2], policy[i+3], policy[i+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0152b199697503339f0fd7f0f935472",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 2 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8fb50a4a7886296d52685a540d2c162",
     "grade": false,
     "grade_id": "cell-e0de56802818cf9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (8 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the stochastic environment presented in exercise 2. A second piece of gold has been placed on grid square 12. The terminal state is reached only when **all** pieces of gold are collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. These arrays should specify the expected return and an optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "635f8795634a99a01b6cc3e5f28ecb3a",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.65103993864426 11.796034329270997 13.008487564535121 10.74288293365086 12.970487142269317\n",
      "11.186135297996916 12.329323720408519 12.512146398880807 0.0 10.615685562832166\n",
      "12.287027476377382 13.593656375413076 12.480726592729345 12.41930415713496 11.199744275232904\n",
      "11.175228371177273 12.35165961880423 13.590922916549713 12.281222174794424 11.051284995299435\n",
      "10.064098056432398 11.174882706353376 12.280459844725018 11.108164762112207 9.993893663954541\n",
      "\n",
      "\n",
      "e e e w w\n",
      "e s s n n\n",
      "e e w w w\n",
      "e n n w w\n",
      "n n n n w\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "import numpy as np\n",
    "\n",
    "class Stochastic2Gold:\n",
    "    def __init__(self):\n",
    "        # gridworld dimensions\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "\n",
    "        # item positions - 2 gold\n",
    "        self.bomb_positions = [18]\n",
    "        self.gold_positions_2 = [12, 23]\n",
    "        \n",
    "        # item positions - 1 gold at 23\n",
    "        self.gold_positions_23 = [23]\n",
    "        \n",
    "        # item positions - 1 gold at 12\n",
    "        self.gold_positions_12 = [12]\n",
    "        self.terminal_states = [self.bomb_positions, [self.bomb_positions[0], self.gold_positions_23[0]], [self.bomb_positions[0], self.gold_positions_12[0]]]\n",
    "        \n",
    "        \"\"\"\n",
    "        rewards, values and policy work as a list of 3 nested lists, each of size 25, where the 1st list is\n",
    "        state with both gold, 2nd is state with 1 gold at 23 and 3rd is state with 1 gold in 12\n",
    "        \"\"\"\n",
    "        \n",
    "        # reward is -1 for moving into grid square with an additional reward of +/- 10 for the terminal states\n",
    "        self.rewards = [[-1 for _ in range(0,self.num_cells)] for _ in range(0,3)]\n",
    "        self.rewards[0][self.bomb_positions[0]] += -10\n",
    "        self.rewards[1][self.bomb_positions[0]] += -10\n",
    "        self.rewards[2][self.bomb_positions[0]] += -10\n",
    "        self.rewards[0][self.gold_positions_2[0]] += 10\n",
    "        self.rewards[0][self.gold_positions_2[1]] += 10\n",
    "        self.rewards[1][self.gold_positions_23[0]] += 10\n",
    "        self.rewards[2][self.gold_positions_12[0]] += 10\n",
    "\n",
    "        self.available_actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "\n",
    "        # state values are initially zero and policy has been initialised with optimal policy \"n\"\n",
    "        self.v = [[0 for _ in range(0,self.num_cells)] for _ in range(0,3)]\n",
    "        self.policy = [[\"n\" for _ in range(0,self.num_cells)] for _ in range(0,3)]\n",
    "        self.theta = 1e-10\n",
    "        self.gamma = 1\n",
    "        # probability of agent not moving as intended is 0.2\n",
    "        self.unintended = 0.2\n",
    "        self.intended = 1 - 0.2\n",
    "\n",
    "    def find_max_v_action(self, gold_state, square):\n",
    "        \"\"\"\n",
    "        uses the values and rewards for neighbouring squares, and returns action corresponding to \n",
    "        the highest value. In stochastic environment, value of each direction is 0.8 probability \n",
    "        intended action and 0.2 random action (with 0.25 chance of each of the 4 actions\n",
    "        \"\"\"\n",
    "        neighbour_v, neighbour_r = self.neighbour_v_r_values(gold_state, square)\n",
    "        north = neighbour_r[0] + self.gamma * neighbour_v[0]\n",
    "        east = neighbour_r[1] + self.gamma * neighbour_v[1]\n",
    "        south = neighbour_r[2] + self.gamma * neighbour_v[2]\n",
    "        west = neighbour_r[3] + self.gamma * neighbour_v[3]\n",
    "        # for stochastic environment, 0.8 prob of intended action and 0.2 prob of random action\n",
    "        st_north = self.intended*north+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_east = self.intended*east+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_south = self.intended*south+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        st_west = self.intended*west+self.unintended*(0.25*north+0.25*east+0.25*south+0.25*west)\n",
    "        max_v = max([st_north, st_east, st_south, st_west])\n",
    "        action_index = [st_north, st_east, st_south, st_west].index(max_v)\n",
    "        return max_v, action_index\n",
    "\n",
    "    def neighbour_v_r_values(self, gold_state, square):\n",
    "        \"\"\"\n",
    "        returns 2 lists for value and rewards for neighbouring squares. In state with both golds, neighbouring\n",
    "        squares to 12 and 23 calculate value for corresponding square in state where the gold on that\n",
    "        square has been collected.\n",
    "        \"\"\"\n",
    "        if gold_state == 0 and (square == 11 or square == 13 or square == 17 or square == 7):\n",
    "            if square == 11:\n",
    "                v_n = self.v[gold_state][square + self.num_cols]\n",
    "                r_n = self.rewards[gold_state][square + self.num_cols]\n",
    "                # when moving east, value at square 12 from v[1][12]\n",
    "                # but reward from v[0][12] because reward of +10 must be collected first before transitioning\n",
    "                v_e = self.v[gold_state + 1][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "            if square == 13:\n",
    "                v_n = self.v[gold_state][square + self.num_cols]\n",
    "                r_n = self.rewards[gold_state][square + self.num_cols]\n",
    "                v_e = self.v[gold_state][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state + 1][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "            if square == 17:\n",
    "                v_n = self.v[gold_state][square + self.num_cols]\n",
    "                r_n = self.rewards[gold_state][square + self.num_cols]\n",
    "                v_e = self.v[gold_state][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "                v_s = self.v[gold_state + 1][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "            if square == 7:\n",
    "                v_n = self.v[gold_state + 1][square + self.num_cols]\n",
    "                r_n = self.rewards[gold_state][square + self.num_cols]\n",
    "                v_e = self.v[gold_state][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "        elif gold_state == 0 and (square == 22 or square == 24):\n",
    "            if square == 22:\n",
    "                v_n = self.v[gold_state][square]\n",
    "                r_n = self.rewards[gold_state][square]\n",
    "                v_e = self.v[gold_state + 2][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "            if square == 24:\n",
    "                v_n = self.v[gold_state][square]\n",
    "                r_n = self.rewards[gold_state][square]\n",
    "                v_e = self.v[gold_state][square]\n",
    "                r_e = self.rewards[gold_state][square]\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "                v_w = self.v[gold_state + 2][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "        else:\n",
    "            if (square + self.num_cols) < self.num_cells:\n",
    "                v_n = self.v[gold_state][square + self.num_cols]\n",
    "                r_n = self.rewards[gold_state][square + self.num_cols]\n",
    "            else:\n",
    "                v_n = self.v[gold_state][square]\n",
    "                # with both golds, square 23 not terminal\n",
    "                # if hitting wall in 23, reward is -1 as gold is not collected without transitioning here\n",
    "                if gold_state == 0 and square == 23:\n",
    "                    r_n = -1\n",
    "                else:\n",
    "                    r_n = self.rewards[gold_state][square]\n",
    "            if (square + 1) % self.num_cols > 0:\n",
    "                v_e = self.v[gold_state][square + 1]\n",
    "                r_e = self.rewards[gold_state][square + 1]\n",
    "            else:\n",
    "                v_e = self.v[gold_state][square]\n",
    "                r_e = self.rewards[gold_state][square]\n",
    "            if (square - self.num_cols) >= 0:\n",
    "                v_s = self.v[gold_state][square - self.num_cols]\n",
    "                r_s = self.rewards[gold_state][square - self.num_cols]\n",
    "            else:\n",
    "                v_s = self.v[gold_state][square]\n",
    "                r_s = self.rewards[gold_state][square]\n",
    "            if (square - 1) % self.num_cols < self.num_cols - 1:\n",
    "                v_w = self.v[gold_state][square - 1]\n",
    "                r_w = self.rewards[gold_state][square - 1]\n",
    "            else:\n",
    "                v_w = self.v[gold_state][square]\n",
    "                r_w = self.rewards[gold_state][square]\n",
    "        return [v_n, v_e, v_s, v_w], [r_n, r_e, r_s, r_w]\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        applies value iteration algorithm to iterate over all non-terminal states until delta < theta.\n",
    "        Each iteration loops over all 3 states until convergence.\n",
    "        \"\"\"\n",
    "        delta = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for b in range(0, len(self.v)):\n",
    "                for s in range(0, self.num_cells):\n",
    "                    if s not in self.terminal_states[b]:\n",
    "                        state_v = self.v[b][s]\n",
    "                        max_v, action = self.find_max_v_action(b, s)\n",
    "                        self.v[b][s] = max_v\n",
    "                        self.policy[b][s] = self.available_actions[action]\n",
    "                        delta = max([delta, abs(self.v[b][s] - state_v)])       \n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "        return np.array(self.v[0]), np.array(self.policy[0])\n",
    "\n",
    "env3 = Stochastic2Gold()\n",
    "v, policy = env3.value_iteration()\n",
    "\n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(v[i], v[i+1], v[i+2], v[i+3], v[i+4]))\n",
    "print(\"\\n\")\n",
    "for i in range(20, -5, -5):\n",
    "    print(\"{} {} {} {} {}\".format(policy[i], policy[i+1], policy[i+2], policy[i+3], policy[i+4]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf587ef9d694513182a8ca1c721200d7",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
